{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5502b856-327a-4572-9716-0944517e07ed",
   "metadata": {},
   "source": [
    "# Direct access to the zarr data on s3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f518b838-c509-4331-b678-0432e7467f00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 885 ms, sys: 191 ms, total: 1.08 s\n",
      "Wall time: 982 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import fsspec\n",
    "import s3fs\n",
    "import xarray\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "fs_s3 = s3fs.S3FileSystem(\n",
    "      anon=False,\n",
    "      key='AKIAIOSFODNN7EXAMPLE',\n",
    "      secret='wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY',\n",
    "      endpoint_url='http://localhost:9444/s3/'\n",
    "   )\n",
    "\n",
    "\n",
    "s3_url_netcdf = 's3://data/SWOT_L2_LR_PreCalSSH_Expert_002_086_20230814T031152_20230814T040129_PIA1_01.nc'\n",
    "s3_url_netcdf_index = 's3://data/SWOT_L2_LR_PreCalSSH_Expert_002_086_20230814T031152_20230814T040129_PIA1_01_indexchunk.nc'\n",
    "s3_url_zarr = \"s3://zarrdata/SWOT_L2_LR_PreCalSSH_Expert_002_086_20230814T031152_20230814T040129_PIA1_01.zarr\"\n",
    "\n",
    "# Read One vector of 70 values\n",
    "slice1 = ((slice(6000, 6001), slice(0, 69)))\n",
    "variable='ssh_karin'\n",
    "\n",
    "# Read 2000 vectors of 70 values\n",
    "slice1 = ((slice(6000, 8000), slice(0, 69)))\n",
    "variable='ssh_karin'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff4eba5-e827-4d88-b4d7-d4bcd6bbba8f",
   "metadata": {},
   "source": [
    "## Get netCDF file with xarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "901fd650-a724-4e5a-8435-630b54735d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61.284000000000006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.DataArray 'ssh_karin' (num_lines: 2000, num_pixels: 69)>\n",
      "array([[     nan,      nan,  61.1774, ...,  57.4727,  57.4768,      nan],\n",
      "       [     nan,      nan,  61.2194, ...,  57.4146,  57.4277,      nan],\n",
      "       [     nan,      nan,  61.2322, ...,  57.3577,  57.3452,      nan],\n",
      "       ...,\n",
      "       [     nan,      nan,      nan, ..., -29.3178, -29.3235,      nan],\n",
      "       [     nan,      nan,      nan, ..., -29.455 , -29.4612,      nan],\n",
      "       [     nan,      nan,      nan, ..., -29.6098, -29.6084,      nan]])\n",
      "Coordinates:\n",
      "    latitude         (num_lines, num_pixels) float64 ...\n",
      "    longitude        (num_lines, num_pixels) float64 ...\n",
      "    latitude_nadir   (num_lines) float64 ...\n",
      "    longitude_nadir  (num_lines) float64 ...\n",
      "Dimensions without coordinates: num_lines, num_pixels\n",
      "Attributes:\n",
      "    long_name:      sea surface height\n",
      "    standard_name:  sea surface height above reference ellipsoid\n",
      "    units:          m\n",
      "    quality_flag:   ssh_karin_qual\n",
      "    valid_min:      -15000000\n",
      "    valid_max:      150000000\n",
      "    comment:        Fully corrected sea surface height measured by KaRIn. The...\n",
      "CPU times: user 1.8 s, sys: 286 ms, total: 2.09 s\n",
      "Wall time: 2.58 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with fs_s3.open(s3_url_netcdf, mode='rb') as f:\n",
    "    with xarray.open_dataset(f, engine='h5netcdf') as dataset:\n",
    "        data = dataset[variable][slice1]\n",
    "        #print(data)\n",
    "        print(data.max().values)\n",
    "\n",
    "print(data)\n",
    "ref_data = data.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48468bbd-5025-4069-816b-795a97d79ea1",
   "metadata": {},
   "source": [
    "## Get netCDF with h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de987101-40f0-4ae2-be91-9412afc1b958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61.284000000000006\n",
      "Elapsed time: 112.54ms\n",
      "[[     nan      nan  61.1774 ...  57.4727  57.4768      nan]\n",
      " [     nan      nan  61.2194 ...  57.4146  57.4277      nan]\n",
      " [     nan      nan  61.2322 ...  57.3577  57.3452      nan]\n",
      " ...\n",
      " [     nan      nan      nan ... -29.3178 -29.3235      nan]\n",
      " [     nan      nan      nan ... -29.455  -29.4612      nan]\n",
      " [     nan      nan      nan ... -29.6098 -29.6084      nan]]\n",
      "CPU times: user 59.7 ms, sys: 17.3 ms, total: 77.1 ms\n",
      "Wall time: 116 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Read the netcdf dataset without the use of the index (fsspec and h5py style)\n",
    "\n",
    "import time\n",
    "import h5py as h5\n",
    "import numpy\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Open the netCDF dataset\n",
    "with fs_s3.open(s3_url_netcdf, mode='rb') as f:\n",
    "    with h5.File(f) as ds:\n",
    "        # Access to h5py low-level API to have a direct access to the compressed data\n",
    "        data = ds[variable][slice1]\n",
    "        liste_att = ds[variable].attrs.keys()\n",
    "        if '_FillValue' in liste_att:\n",
    "            fillvalue = ds[variable].attrs['_FillValue'][0]\n",
    "        else:\n",
    "            fillvalue = False\n",
    "        if 'scale_factor' in liste_att:\n",
    "            scale_factor = ds[variable].attrs['scale_factor'][0]\n",
    "        else:\n",
    "            scale_factor = 1\n",
    "        if 'offset' in liste_att:\n",
    "            offset = ds[variable].attrs['offset'][0]\n",
    "        else:\n",
    "            offset = 0\n",
    "        if fillvalue:\n",
    "            data = numpy.where(data==fillvalue, numpy.nan, data)*scale_factor + offset\n",
    "        else:\n",
    "            data = data*scale_factor + offset\n",
    "        print(numpy.nanmax(data))\n",
    "\n",
    "end_time = time.time() - start_time\n",
    "print('Elapsed time: %.2fms' % (end_time*1000))\n",
    "\n",
    "# Check the data decompressed\n",
    "print(data)\n",
    "assert(numpy.allclose(data, ref_data, equal_nan=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e2ce20-2654-4e8e-817c-276936941362",
   "metadata": {},
   "source": [
    "## Get zarr file with xarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b107cdce-4ac6-45d8-bd98-49756dc7b0ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.DataArray 'ssh_karin' (num_lines: 2000, num_pixels: 69)>\n",
      "dask.array<getitem, shape=(2000, 69), dtype=float64, chunksize=(1401, 35), chunktype=numpy.ndarray>\n",
      "Coordinates:\n",
      "    latitude         (num_lines, num_pixels) float64 dask.array<chunksize=(1401, 35), meta=np.ndarray>\n",
      "    latitude_nadir   (num_lines) float64 dask.array<chunksize=(2000,), meta=np.ndarray>\n",
      "    longitude        (num_lines, num_pixels) float64 dask.array<chunksize=(1401, 35), meta=np.ndarray>\n",
      "    longitude_nadir  (num_lines) float64 dask.array<chunksize=(2000,), meta=np.ndarray>\n",
      "Dimensions without coordinates: num_lines, num_pixels\n",
      "Attributes:\n",
      "    comment:        Fully corrected sea surface height measured by KaRIn. The...\n",
      "    long_name:      sea surface height\n",
      "    quality_flag:   ssh_karin_qual\n",
      "    standard_name:  sea surface height above reference ellipsoid\n",
      "    units:          m\n",
      "    valid_max:      150000000\n",
      "    valid_min:      -15000000\n",
      "61.284000000000006\n",
      "CPU times: user 299 ms, sys: 13.6 ms, total: 313 ms\n",
      "Wall time: 394 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "mapper = fs_s3.get_mapper(s3_url_zarr)\n",
    "\n",
    "with xarray.open_zarr(store=mapper, consolidated=True) as zarr_ds:\n",
    "    data = zarr_ds[variable][slice1]\n",
    "    print(data)\n",
    "    print(data.max().values)\n",
    "\n",
    "\n",
    "#with fs_s3.open(s3_url_zarr, mode='rb') as f:\n",
    "#    with xarray.open_dataset(f, engine='zarr') as dataset:\n",
    "#        data = dataset[variable][slice1]\n",
    "#        #print(data)\n",
    "#        print(data.max().values)\n",
    "assert(numpy.allclose(data, ref_data, equal_nan=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f8d491-ecc5-4c9d-9d84-8b06e6efc4d3",
   "metadata": {},
   "source": [
    "## Get zarr file with zarr python lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6299aca-6524-4ee8-8fca-9ce12455c56f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "get attributes\n",
      "dict_keys(['_ARRAY_DIMENSIONS', 'comment', 'coordinates', 'long_name', 'quality_flag', 'scale_factor', 'standard_name', 'units', 'valid_max', 'valid_min'])\n",
      "No fillvalue found\n",
      "61.284000000000006\n",
      "CPU times: user 66.4 ms, sys: 14.4 ms, total: 80.8 ms\n",
      "Wall time: 290 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import zarr\n",
    "\n",
    "mapper = fs_s3.get_mapper(s3_url_zarr)\n",
    "print(\"test\")\n",
    "with zarr.open(mapper) as zarr_ds:\n",
    "    data = zarr_ds[variable][slice1]\n",
    "    print(\"get attributes\")\n",
    "    liste_att = zarr_ds[variable].attrs.keys()\n",
    "    print(liste_att)\n",
    "    if '_FillValue' in liste_att:\n",
    "        print(\"get fillvalue\")\n",
    "        fillvalue = zarr_ds[variable].attrs['_FillValue'][0]\n",
    "    else:\n",
    "        print(\"No fillvalue found\")\n",
    "        fillvalue = False\n",
    "        \n",
    "    if 'scale_factor' in liste_att:\n",
    "        scale_factor = zarr_ds[variable].attrs['scale_factor']\n",
    "    else:\n",
    "        scale_factor = 1\n",
    "    if 'offset' in liste_att:\n",
    "        offset = zarr_ds[variable].attrs['offset'][0]\n",
    "    else:\n",
    "        offset = 0\n",
    "    # Attention problème dans la récupération des FV, ce n'est pas un attribut classique \n",
    "    # dans le format zarr comme cela l'est dans le format netCDF\n",
    "    # TODO find a way to get the _FillValue attribute\n",
    "    # Topic : https://github.com/pydata/xarray/issues/5475\n",
    "    fillvalue=2147483647\n",
    "    if fillvalue:\n",
    "        data = numpy.where(data==fillvalue, numpy.nan, data)*scale_factor + offset\n",
    "    else:\n",
    "        data = data*scale_factor + offset\n",
    "    print(numpy.nanmax(data))\n",
    "\n",
    "#with fs_s3.open(s3_url_zarr, mode='rb') as f:\n",
    "#    with xarray.open_dataset(f, engine='zarr') as dataset:\n",
    "#        data = dataset[variable][slice1]\n",
    "#        #print(data)\n",
    "#        print(data.max().values)\n",
    "assert(numpy.allclose(data, ref_data, equal_nan=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd2411d-a601-4dc3-9c7b-9014d8def5a4",
   "metadata": {},
   "source": [
    "# Access to multiple vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1cd1c8ca-1fa3-4163-a8ed-8d6893f7cc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read 9 vectors of 50 values in the middle of the data\n",
    "slice1 = ((slice(1000, 10000, 1000), slice(10, 60)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8361310-83be-45ed-9828-6ef8e3784198",
   "metadata": {},
   "source": [
    "## Direct acces to netCDF file via h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "521e9eae-fbee-4d69-b6af-a94fcb0fe022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60.8926\n",
      "Elapsed time: 83.64ms\n",
      "(9, 50)\n",
      "CPU times: user 60.7 ms, sys: 7.45 ms, total: 68.2 ms\n",
      "Wall time: 83.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Read the netcdf dataset without the use of the index (fsspec and h5py style)\n",
    "\n",
    "import time\n",
    "import h5py as h5\n",
    "import numpy\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Open the netCDF dataset\n",
    "with fs_s3.open(s3_url_netcdf, mode='rb') as f:\n",
    "    with h5.File(f) as ds:\n",
    "        # Access to h5py low-level API to have a direct access to the compressed data\n",
    "        data = ds[variable][slice1]\n",
    "        liste_att = ds[variable].attrs.keys()\n",
    "        if '_FillValue' in liste_att:\n",
    "            fillvalue = ds[variable].attrs['_FillValue'][0]\n",
    "        else:\n",
    "            fillvalue = False\n",
    "        if 'scale_factor' in liste_att:\n",
    "            scale_factor = ds[variable].attrs['scale_factor'][0]\n",
    "        else:\n",
    "            scale_factor = 1\n",
    "        if 'offset' in liste_att:\n",
    "            offset = ds[variable].attrs['offset'][0]\n",
    "        else:\n",
    "            offset = 0\n",
    "        if fillvalue:\n",
    "            data = numpy.where(data==fillvalue, numpy.nan, data)*scale_factor + offset\n",
    "        else:\n",
    "            data = data*scale_factor + offset\n",
    "        print(numpy.nanmax(data))\n",
    "\n",
    "end_time = time.time() - start_time\n",
    "print('Elapsed time: %.2fms' % (end_time*1000))\n",
    "print(numpy.shape(data))\n",
    "\n",
    "ref_data2 = data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff819c2d-ea33-400a-a532-d1dbe64ab744",
   "metadata": {},
   "source": [
    "## Access to zarr file via zarr lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "654339a1-d77a-470d-b7d2-b0eb769ed585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "get attributes\n",
      "dict_keys(['_ARRAY_DIMENSIONS', 'comment', 'coordinates', 'long_name', 'quality_flag', 'scale_factor', 'standard_name', 'units', 'valid_max', 'valid_min'])\n",
      "No fillvalue found\n",
      "60.8926\n",
      "CPU times: user 77.4 ms, sys: 11.7 ms, total: 89.1 ms\n",
      "Wall time: 224 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import zarr\n",
    "\n",
    "mapper = fs_s3.get_mapper(s3_url_zarr)\n",
    "print(\"test\")\n",
    "with zarr.open(mapper) as zarr_ds:\n",
    "    data = zarr_ds[variable][slice1]\n",
    "    print(\"get attributes\")\n",
    "    liste_att = zarr_ds[variable].attrs.keys()\n",
    "    print(liste_att)\n",
    "    if '_FillValue' in liste_att:\n",
    "        print(\"get fillvalue\")\n",
    "        fillvalue = zarr_ds[variable].attrs['_FillValue'][0]\n",
    "    else:\n",
    "        print(\"No fillvalue found\")\n",
    "        fillvalue = False\n",
    "        \n",
    "    if 'scale_factor' in liste_att:\n",
    "        scale_factor = zarr_ds[variable].attrs['scale_factor']\n",
    "    else:\n",
    "        scale_factor = 1\n",
    "    if 'offset' in liste_att:\n",
    "        offset = zarr_ds[variable].attrs['offset'][0]\n",
    "    else:\n",
    "        offset = 0\n",
    "    # Attention problème dans la récupération des FV, ce n'est pas un attribut classique \n",
    "    # dans le format zarr comme cela l'est dans le format netCDF\n",
    "    # TODO find a way to get the _FillValue attribute\n",
    "    # Topic : https://github.com/pydata/xarray/issues/5475\n",
    "    fillvalue=2147483647\n",
    "    if fillvalue:\n",
    "        data = numpy.where(data==fillvalue, numpy.nan, data)*scale_factor + offset\n",
    "    else:\n",
    "        data = data*scale_factor + offset\n",
    "    print(numpy.nanmax(data))\n",
    "\n",
    "#with fs_s3.open(s3_url_zarr, mode='rb') as f:\n",
    "#    with xarray.open_dataset(f, engine='zarr') as dataset:\n",
    "#        data = dataset[variable][slice1]\n",
    "#        #print(data)\n",
    "#        print(data.max().values)\n",
    "assert(numpy.allclose(data, ref_data2, equal_nan=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2062ea-564c-4584-a17a-e69858fcce52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
